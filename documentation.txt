Skip to content
logo
OpenAI Agents SDK
Agents
Search

OpenAI Agents SDK
Intro
Quickstart
Examples
Documentation
Agents
Running agents
Results
Streaming
Tools
Handoffs
Tracing
Context management
Guardrails
Orchestrating multiple agents
Models
Configuring the SDK
Voice agents
Quickstart
Pipelines and workflows
Tracing
API Reference
Agents
Agents module
Agents
Runner
Tools
Results
Streaming events
Handoffs
Lifecycle
Items
Run context
Usage
Exceptions
Guardrails
Model settings
Agent output
Function schema
Model interface
OpenAI Chat Completions model
OpenAI Responses model
Tracing
Tracing module
Creating traces/spans
Traces
Spans
Processor interface
Processors
Scope
Setup
Span data
Util
Voice
Pipeline
Workflow
Input
Result
Pipeline Config
Events
Exceptions
Model
Utils
OpenAIVoiceModelProvider
OpenAI STT
OpenAI TTS
Extensions
Handoff filters
Handoff prompt
Table of contents
Basic configuration
Context
Output types
Handoffs
Dynamic instructions
Lifecycle events (hooks)
Guardrails
Cloning/copying agents
Forcing tool use
Agents
Agents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.

Basic configuration
The most common properties of an agent you'll configure are:

instructions: also known as a developer message or system prompt.
model: which LLM to use, and optional model_settings to configure model tuning parameters like temperature, top_p, etc.
tools: Tools that the agent can use to achieve its tasks.

from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="o3-mini",
    tools=[get_weather],
)
Context
Agents are generic on their context type. Context is a dependency-injection tool: it's an object you create and pass to Runner.run(), that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.


@dataclass
class UserContext:
  uid: str
  is_pro_user: bool

  async def fetch_purchases() -> list[Purchase]:
     return ...

agent = Agent[UserContext](
    ...,
)
Output types
By default, agents produce plain text (i.e. str) outputs. If you want the agent to produce a particular type of output, you can use the output_type parameter. A common choice is to use Pydantic objects, but we support any type that can be wrapped in a Pydantic TypeAdapter - dataclasses, lists, TypedDict, etc.


from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)
Note

When you pass an output_type, that tells the model to use structured outputs instead of regular plain text responses.

Handoffs
Handoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant. This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task. Read more in the handoffs documentation.


from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions."
        "If they ask about booking, handoff to the booking agent."
        "If they ask about refunds, handoff to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)
Dynamic instructions
In most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and async functions are accepted.


def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)
Lifecycle events (hooks)
Sometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the hooks property. Subclass the AgentHooks class, and override the methods you're interested in.

Guardrails
Guardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance. Read more in the guardrails documentation.

Cloning/copying agents
By using the clone() method on an agent, you can duplicate an Agent, and optionally change any properties you like.


pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="o3-mini",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)
Forcing tool use
Supplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting ModelSettings.tool_choice. Valid values are:

auto, which allows the LLM to decide whether or not to use a tool.
required, which requires the LLM to use a tool (but it can intelligently decide which tool).
none, which requires the LLM to not use a tool.
Setting a specific string e.g. my_tool, which requires the LLM to use that specific tool.
Note

If requiring tool use, you should consider setting [Agent.tool_use_behavior] to stop the Agent from running when a tool output is produced. Otherwise, the Agent might run in an infinite loop, where the LLM produces a tool call , and the tool result is sent to the LLM, and this infinite loops because the LLM is always forced to use a tool.

Skip to content
logo
OpenAI Agents SDK
Running agents
Search

OpenAI Agents SDK
Intro
Quickstart
Examples
Documentation
Agents
Running agents
Results
Streaming
Tools
Handoffs
Tracing
Context management
Guardrails
Orchestrating multiple agents
Models
Configuring the SDK
Voice agents
Quickstart
Pipelines and workflows
Tracing
API Reference
Agents
Agents module
Agents
Runner
Tools
Results
Streaming events
Handoffs
Lifecycle
Items
Run context
Usage
Exceptions
Guardrails
Model settings
Agent output
Function schema
Model interface
OpenAI Chat Completions model
OpenAI Responses model
Tracing
Tracing module
Creating traces/spans
Traces
Spans
Processor interface
Processors
Scope
Setup
Span data
Util
Voice
Pipeline
Workflow
Input
Result
Pipeline Config
Events
Exceptions
Model
Utils
OpenAIVoiceModelProvider
OpenAI STT
OpenAI TTS
Extensions
Handoff filters
Handoff prompt
Table of contents
The agent loop
Streaming
Run config
Conversations/chat threads
Exceptions
Running agents
You can run agents via the Runner class. You have 3 options:

Runner.run(), which runs async and returns a RunResult.
Runner.run_sync(), which is a sync method and just runs .run() under the hood.
Runner.run_streamed(), which runs async and returns a RunResultStreaming. It calls the LLM in streaming mode, and streams those events to you as they are received.

from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance.
Read more in the results guide.

The agent loop
When you use the run method in Runner, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.

The runner then runs a loop:

We call the LLM for the current agent, with the current input.
The LLM produces its output.
If the LLM returns a final_output, the loop ends and we return the result.
If the LLM does a handoff, we update the current agent and input, and re-run the loop.
If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.
If we exceed the max_turns passed, we raise a MaxTurnsExceeded exception.
Note

The rule for whether the LLM output is considered as a "final output" is that it produces text output with the desired type, and there are no tool calls.

Streaming
Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the RunResultStreaming will contain the complete information about the run, including all the new outputs produces. You can call .stream_events() for the streaming events. Read more in the streaming guide.

Run config
The run_config parameter lets you configure some global settings for the agent run:

model: Allows setting a global LLM model to use, irrespective of what model each Agent has.
model_provider: A model provider for looking up model names, which defaults to OpenAI.
model_settings: Overrides agent-specific settings. For example, you can set a global temperature or top_p.
input_guardrails, output_guardrails: A list of input or output guardrails to include on all runs.
handoff_input_filter: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in Handoff.input_filter for more details.
tracing_disabled: Allows you to disable tracing for the entire run.
trace_include_sensitive_data: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.
workflow_name, trace_id, group_id: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting workflow_name. The session ID is an optional field that lets you link traces across multiple runs.
trace_metadata: Metadata to include on all traces.
Conversations/chat threads
Calling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:

User turn: user enter text
Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.
At the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.

You can use the base RunResultBase.to_input_list() method to get the inputs for the next turn.


async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
Exceptions
The SDK raises exceptions in certain cases. The full list is in agents.exceptions. As an overview:

AgentsException is the base class for all exceptions raised in the SDK.
MaxTurnsExceeded is raised when the run exceeds the max_turns passed to the run methods.
ModelBehaviorError is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.
UserError is raised when you (the person writing code using the SDK) make an error using the SDK.
InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered is raised when a guardrail is tripped.


Skip to content
logo
OpenAI Agents SDK
Results
Search

OpenAI Agents SDK
Intro
Quickstart
Examples
Documentation
Agents
Running agents
Results
Streaming
Tools
Handoffs
Tracing
Context management
Guardrails
Orchestrating multiple agents
Models
Configuring the SDK
Voice agents
Quickstart
Pipelines and workflows
Tracing
API Reference
Agents
Agents module
Agents
Runner
Tools
Results
Streaming events
Handoffs
Lifecycle
Items
Run context
Usage
Exceptions
Guardrails
Model settings
Agent output
Function schema
Model interface
OpenAI Chat Completions model
OpenAI Responses model
Tracing
Tracing module
Creating traces/spans
Traces
Spans
Processor interface
Processors
Scope
Setup
Span data
Util
Voice
Pipeline
Workflow
Input
Result
Pipeline Config
Events
Exceptions
Model
Utils
OpenAIVoiceModelProvider
OpenAI STT
OpenAI TTS
Extensions
Handoff filters
Handoff prompt
Table of contents
Final output
Inputs for the next turn
Last agent
New items
Other information
Guardrail results
Raw responses
Original input
Results
When you call the Runner.run methods, you either get a:

RunResult if you call run or run_sync
RunResultStreaming if you call run_streamed
Both of these inherit from RunResultBase, which is where most useful information is present.

Final output
The final_output property contains the final output of the last agent that ran. This is either:

a str, if the last agent didn't have an output_type defined
an object of type last_agent.output_type, if the agent had an output type defined.
Note

final_output is of type Any. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.

Inputs for the next turn
You can use result.to_input_list() to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.

Last agent
The last_agent property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.

New items
The new_items property contains the new items generated during the run. The items are RunItems. A run item wraps the raw item generated by the LLM.

MessageOutputItem indicates a message from the LLM. The raw item is the message generated.
HandoffCallItem indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
HandoffOutputItem indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
ToolCallItem indicates that the LLM invoked a tool.
ToolCallOutputItem indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
ReasoningItem indicates a reasoning item from the LLM. The raw item is the reasoning generated.
Other information
Guardrail results
The input_guardrail_results and output_guardrail_results properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.

Raw responses
The raw_responses property contains the ModelResponses generated by the LLM.

Original input
The input property contains the original input you provided to the run method. In most cases you won't need this, but it's available in case you do.

Skip to content
logo
OpenAI Agents SDK
Streaming
Search

OpenAI Agents SDK
Intro
Quickstart
Examples
Documentation
Agents
Running agents
Results
Streaming
Tools
Handoffs
Tracing
Context management
Guardrails
Orchestrating multiple agents
Models
Configuring the SDK
Voice agents
Quickstart
Pipelines and workflows
Tracing
API Reference
Agents
Agents module
Agents
Runner
Tools
Results
Streaming events
Handoffs
Lifecycle
Items
Run context
Usage
Exceptions
Guardrails
Model settings
Agent output
Function schema
Model interface
OpenAI Chat Completions model
OpenAI Responses model
Tracing
Tracing module
Creating traces/spans
Traces
Spans
Processor interface
Processors
Scope
Setup
Span data
Util
Voice
Pipeline
Workflow
Input
Result
Pipeline Config
Events
Exceptions
Model
Utils
OpenAIVoiceModelProvider
OpenAI STT
OpenAI TTS
Extensions
Handoff filters
Handoff prompt
Table of contents
Raw response events
Run item events and agent events
Streaming
Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.

To stream, you can call Runner.run_streamed(), which will give you a RunResultStreaming. Calling result.stream_events() gives you an async stream of StreamEvent objects, which are described below.

Raw response events
RawResponsesStreamEvent are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like response.created, response.output_text.delta, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.

For example, this will output the text generated by the LLM token-by-token.


import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
Run item events and agent events
RunItemStreamEvents are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of "message generated", "tool ran", etc, instead of each token. Similarly, AgentUpdatedStreamEvent gives you updates when the current agent changes (e.g. as the result of a handoff).

For example, this will ignore raw events and stream updates to the user.


import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
